{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Widen jupyter notebook \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from Context_Layer_auto_switch_numpy_untrainable_weights import Context as Ntask\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Seeding\n",
    "from random import randrange \n",
    "import random\n",
    "random.seed(5)\n",
    "tf.set_random_seed(5)\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "# Datasets\n",
    "import dataset_8_logic_gates as data\n",
    "import logic_gate_test\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn off GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_data = data.xor_data\n",
    "xnor_data = data.xnor_data\n",
    "and_data = data.and_data\n",
    "or_data = data.or_data\n",
    "nor_data = data.nor_data\n",
    "nand_data = data.nand_data\n",
    "custom_gate_0_data = data.custom_gate_0_data\n",
    "custom_gate_1_data = data.custom_gate_1_data\n",
    "\n",
    "#List of all data above\n",
    "all_data = data.all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = [\n",
    "    \n",
    "#     xor_data,\n",
    "#     xnor_data,\n",
    "#     and_data,\n",
    "#     #or_data,\n",
    "#     #nor_data,\n",
    "#     #nand_data,\n",
    "#     #custom_gate_0_data\n",
    "    \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTASK_LAYER_IDX = 2\n",
    "num_task_contexts=len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init called\n",
      "build called\n",
      "hardcoed_contexts=False\n"
     ]
    }
   ],
   "source": [
    "inp = Input(2,)\n",
    "x = Dense(20, activation='relu')(inp)\n",
    "x = Ntask(num_task_contexts, hardcoded_contexts=False)(x)\n",
    "#x = Dense(20, activation='relu')(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "context (Context)            (None, 20)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 241\n",
      "Trainable params: 81\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, predictions):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true=labels, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Switching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_loss):\n",
    "\n",
    "    \"\"\"\n",
    "    Swith Ntask layer to better fitting context, when provided better fitting context.\n",
    "    Resets several variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    hot_context_idx = next_context_idx\n",
    "    model.layers[NTASK_LAYER_IDX].set_hot_context( hot_context_idx )\n",
    "\n",
    "    epoch_grads.clear()\n",
    "    cur_epoch_context_loss[hot_context_idx] = 0\n",
    "    #epoch_loss = 0\n",
    "    return hot_context_idx\n",
    "    #return hot_context_idx, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_next_context(next_context_idx, model, context_idx, hot_context_idx):\n",
    "    \n",
    "    \"\"\"\n",
    "    Switch Ntask layer hot context to the next context & update hot_context_idx.\n",
    "    ex: If hot_context_idx := 2 \n",
    "            func call results in:\n",
    "                hot_context_idx := 3\n",
    "                the hot context is now the context at idx 3\n",
    "    \"\"\"\n",
    "    \n",
    "    hot_context_idx = next_context_idx\n",
    "    model.layers[NTASK_LAYER_IDX].set_hot_context( hot_context_idx )\n",
    "\n",
    "    return hot_context_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_context_loss(gradients, model, ntask_layer_idx_in_model, idx_of_next_layer_bias_gradient, idx_of_next_layer_weights_in_get_weights_call=0):\n",
    "    \"\"\"\n",
    "    IMPORTANT: \n",
    "    1) Assumes no use of activation function on Ntask layer\n",
    "    2) Assumes that the layer following the Ntask layer:\n",
    "        a) Is a Dense layer\n",
    "        b) Is using bias \n",
    "           — ex: Dense(20, ... , use_bias=True) \n",
    "           — note Keras Dense layer uses bias by default if no value is given for use_bias param\n",
    "    3) Assumes index of the next layer's gradient is known within the gradients list returned from gradient tape in a tape.gradient call\n",
    "    \"\"\"\n",
    "    \n",
    "    delta_at_next_layer = gradients[idx_of_next_layer_bias_gradient]\n",
    "    transpose_of_weights_at_next_layer = tf.transpose(model.layers[ntask_layer_idx_in_model+1].get_weights()[idx_of_next_layer_weights_in_get_weights_call])\n",
    "      \n",
    "    # Calculate delta at ntask layer\n",
    "    context_delta = np.dot( delta_at_next_layer, transpose_of_weights_at_next_layer ).astype(np.float)\n",
    "    \n",
    "    # Calculate Context Error\n",
    "    # Keras MSE must have both args be arrs of floats, if one or both are arrs of ints, the output will be rounded to an int\n",
    "    context_loss = tf.keras.losses.mean_squared_error(np.zeros(len(context_delta)), context_delta)\n",
    "\n",
    "    return context_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_forward_pass(dataset, model, epoch_grads, context_idx, cur_epoch_context_loss, all_epoch_losses):\n",
    " \n",
    "\n",
    "    sum_loss = 0\n",
    "    \n",
    "    for x, y in dataset:   \n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:            \n",
    "            predictions = model(x, training=True) # forward pass\n",
    "            pred_loss = loss_fn(y, predictions)   # get loss\n",
    "\n",
    "            \n",
    "        sum_loss += pred_loss\n",
    "        \n",
    "        gradients = tape.gradient(pred_loss, model.trainable_variables)\n",
    "        epoch_grads.append(gradients)\n",
    "\n",
    "        context_loss = calc_context_loss(gradients, \n",
    "                                           model, \n",
    "                                           context_idx, \n",
    "                                           idx_of_next_layer_bias_gradient=3, \n",
    "                                           idx_of_next_layer_weights_in_get_weights_call=0)\n",
    "        \n",
    "        \n",
    "        cur_epoch_context_loss[hot_context_idx] += context_loss\n",
    "\n",
    "    avg_loss_for_epoch = sum_loss / len(dataset)\n",
    "\n",
    "    all_epoch_losses.append(avg_loss_for_epoch)\n",
    "\n",
    "    return  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_grads_and_update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model):\n",
    "\n",
    "    # ORINGIAL\n",
    "    moving_avg_context_loss[hot_context_idx] = (moving_avg_context_loss[hot_context_idx] + cur_epoch_context_loss[hot_context_idx]) / 2.0\n",
    "    \n",
    "    diff_errs = [0 for x in range(num_task_contexts)]\n",
    "\n",
    "    #Backprop\n",
    "    for grads in epoch_grads:\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    num_epochs_without_learning = 0\n",
    "\n",
    "    return diff_errs, num_epochs_without_learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 1.0\n",
    "moving_avg_context_loss = [thresh for x in range(num_task_contexts)]\n",
    "hot_context_idx = 0\n",
    "diff_errs = [thresh for x in range(num_task_contexts)]\n",
    "cur_epoch_context_loss = [0 for x in range(num_task_contexts)]\n",
    "all_epoch_losses = []\n",
    "context_switch_threshold = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_without_learning = 0\n",
    "\n",
    "def train(model, dataset, n_epochs, debug, plotting_debug, num_task_contexts):\n",
    "    \n",
    "    global all_epoch_losses\n",
    "\n",
    "    global hot_context_idx\n",
    "    \n",
    "    global cur_epoch_context_loss\n",
    "    global moving_avg_context_loss\n",
    "    global diff_errs\n",
    "    global context_switch_threshold\n",
    "    \n",
    "    global num_epochs_without_learning\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        #avg_loss_for_epoch = 0\n",
    "        epoch_grads = []\n",
    "        cur_epoch_context_loss[hot_context_idx] = 0\n",
    "        \n",
    "        \n",
    "        #======================#\n",
    "        # General Forward Pass #      \n",
    "        #----------------------#\n",
    "        custom_forward_pass(dataset, model, epoch_grads, NTASK_LAYER_IDX, cur_epoch_context_loss, all_epoch_losses)\n",
    "\n",
    "        \n",
    "        # If gone num_task_contexts epochs without learning on a context\n",
    "        # And No Context Fits Well, Need To Pick Best Fit\n",
    "        if num_epochs_without_learning >= num_task_contexts:\n",
    "            \n",
    "            \n",
    "            # Find Best Fitting Context For Current Task\n",
    "            # bc went over all the contexts on the current task\n",
    "            # the diff_errs accurately tells us which context fits best for this task\n",
    "            next_context_idx = diff_errs.index(max(diff_errs))\n",
    "            \n",
    "            # Best fitting context is the one that just had a forward pass performed on it\n",
    "            # So -> Apply the Gradients\n",
    "            # Continue to next epoch \n",
    "            if next_context_idx == hot_context_idx:\n",
    "                diff_errs, num_epochs_without_learning = apply_grads_and_update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model)\n",
    "                continue\n",
    "            \n",
    "            # Current Context does not have the best fit, so don't apply its grads\n",
    "            # Now that the best fitting context has been found, train on it\n",
    "            else:\n",
    "                \n",
    "                # Switch to best fitting Context\n",
    "                #hot_context_idx, epoch_loss = switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_error, epoch_loss)\n",
    "                hot_context_idx = switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_loss)\n",
    "\n",
    "                #==================#\n",
    "                # General Training #\n",
    "                #==================#\n",
    "                custom_forward_pass(dataset, model, epoch_grads, NTASK_LAYER_IDX, cur_epoch_context_loss, all_epoch_losses)\n",
    "\n",
    "            \n",
    "                diff_errs, num_epochs_without_learning = apply_grads_and_update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model)\n",
    "                continue\n",
    "        \n",
    "        next_context_idx = (hot_context_idx + 1) % len(moving_avg_context_loss)\n",
    "                \n",
    "        \n",
    "        diff_errs[hot_context_idx] = moving_avg_context_loss[hot_context_idx] - cur_epoch_context_loss[hot_context_idx]\n",
    "        \n",
    "        ##########################\n",
    "        # Should this happen if right context too?\n",
    "        # If so, where should this occur?\n",
    "        ###########################\n",
    "        if diff_errs[hot_context_idx] < context_switch_threshold:            \n",
    "            num_epochs_without_learning += 1\n",
    "            \n",
    "            \n",
    "            # TRACK WHICH EPOCH THIS BOCK HAPPENS IN\n",
    "            # PLOT ON GRAPH\n",
    "            # OVER TIME, THIS BLOCK SHOULD GET EXECUTED LESS\n",
    "            # SO WHEN TASKS ARE LEARNED WELL, SHOULD SEE THIS EXECTUED ONLY ONCE, FIRST EPOCH OF NEW TASK\n",
    "            \n",
    "            # find best context\n",
    "            # if it is my current context -> learn\n",
    "            # else switch to best \n",
    "            \n",
    "            #WE DONT LOSE EPOCHS HERE BY DOING IT THIS WAY            \n",
    "            \n",
    "            hot_context_idx = switch_to_next_context(next_context_idx, model, NTASK_LAYER_IDX, hot_context_idx)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        #--------------------------    \n",
    "        # didnt switch, so apply grads\n",
    "        else:\n",
    "            #update moving avg err\n",
    "            moving_avg_context_loss[hot_context_idx] = (moving_avg_context_loss[hot_context_idx] + cur_epoch_context_loss[hot_context_idx]) / 2.0\n",
    "            \n",
    "            for grads in epoch_grads:\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Randomly on tasks for N cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_in_cycles(all_data, model, num_tasks, num_cycles, num_epochs):\n",
    "    \n",
    "    prev_task_data_idx = num_tasks-1    # init first choice as last task\n",
    "    order_of_tasks_learned_on = []\n",
    "\n",
    "    #FYI this is correct:\n",
    "    # same as for c in range(cycle): for t in range num_tasks:\n",
    "    for i in tqdm(range( num_cycles * num_tasks )):\n",
    "\n",
    "        cur_task_data_idx = randrange(num_tasks)\n",
    "\n",
    "        while cur_task_data_idx == prev_task_data_idx:\n",
    "            cur_task_data_idx = randrange(num_tasks)\n",
    "\n",
    "        cur_task_data = all_data[cur_task_data_idx]\n",
    "        #print(\"Currently training on data from all_data[ \", cur_task_data_idx)\n",
    "        order_of_tasks_learned_on.append(cur_task_data_idx)\n",
    "\n",
    "\n",
    "        \n",
    "        train(model, cur_task_data, n_epochs=num_epochs, debug=False, plotting_debug=False, num_task_contexts=num_tasks)\n",
    "\n",
    "        prev_task_data_idx = cur_task_data_idx\n",
    "        \n",
    "    return order_of_tasks_learned_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model):\n",
    "\n",
    "    # ORINGIAL\n",
    "    moving_avg_context_loss[hot_context_idx] = (moving_avg_context_loss[hot_context_idx] + cur_epoch_context_loss[hot_context_idx]) / 2.0\n",
    "    \n",
    "    diff_errs = [0 for x in range(num_task_contexts)]\n",
    "\n",
    "    #Backprop\n",
    "    #for grads in epoch_grads:\n",
    "    #    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    num_epochs_without_learning = 0\n",
    "\n",
    "    return diff_errs, num_epochs_without_learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accuracies = []\n",
    "double_epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "global test_switch_epoch_counter\n",
    "\n",
    "test_switch_epoch_counter = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass(dataset, model, epoch_grads, context_idx, cur_epoch_context_loss, all_epoch_losses):\n",
    " \n",
    "    global epoch_accuracies\n",
    "    \n",
    "    global test_switch_epoch_counter\n",
    "    \n",
    "    if  ( not np.isnan(test_switch_epoch_counter) ) and ( test_switch_epoch_counter < num_task_contexts-1 ):\n",
    "        test_switch_epoch_counter += 1\n",
    "        #append switch\n",
    "        epoch_accuracies.append(\"switch\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    if test_switch_epoch_counter >= num_task_contexts-1:\n",
    "        test_switch_epoch_counter = np.nan\n",
    "    \n",
    "    \n",
    "    sum_loss = 0\n",
    "    \n",
    "    acc = 0.0\n",
    "    #print(\"acc at top of loop:\", acc)\n",
    "    #print()\n",
    "    for x, y in dataset:   \n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:            \n",
    "            predictions = model(x, training=True) # forward pass\n",
    "            pred_loss = loss_fn(y, predictions)   # get loss\n",
    "\n",
    "        rounded_pred = int(tf.math.round(predictions).numpy()[0][0])\n",
    "            \n",
    "        #print(\"predictions:\", rounded_pred )   \n",
    "        #print(\"label:\", y)\n",
    "        \n",
    "        #print()\n",
    "        \n",
    "        if rounded_pred == y:\n",
    "            acc += 1.0\n",
    "            \n",
    "        sum_loss += pred_loss\n",
    "        \n",
    "        gradients = tape.gradient(pred_loss, model.trainable_variables)\n",
    "        epoch_grads.append(gradients)\n",
    "\n",
    "        context_loss = calc_context_loss(gradients, \n",
    "                                           model, \n",
    "                                           context_idx, \n",
    "                                           idx_of_next_layer_bias_gradient=3, \n",
    "                                           idx_of_next_layer_weights_in_get_weights_call=0)\n",
    "        \n",
    "        \n",
    "        cur_epoch_context_loss[hot_context_idx] += context_loss\n",
    "    \n",
    "    acc /= len(dataset)\n",
    "    #print(\"acc at end of epoch:\", acc)\n",
    "\n",
    "    epoch_accuracies.append(acc)\n",
    "    \n",
    "    avg_loss_for_epoch = sum_loss / len(dataset)\n",
    "\n",
    "    all_epoch_losses.append(avg_loss_for_epoch)\n",
    "\n",
    "    return  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General dynamic testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_without_learning = 0\n",
    "\n",
    "def test(model, dataset, n_epochs, debug, plotting_debug, num_task_contexts):\n",
    "\n",
    "    #global weights_before\n",
    "    #global weights_after\n",
    "    \n",
    "    global double_epoch_count\n",
    "    \n",
    "    #global a\n",
    "    #global b\n",
    "    #global c\n",
    "    #global d\n",
    "    \n",
    "    global all_epoch_losses\n",
    "\n",
    "    global hot_context_idx\n",
    "    \n",
    "    global cur_epoch_context_loss\n",
    "    global moving_avg_context_loss\n",
    "    global diff_errs\n",
    "    global context_switch_threshold\n",
    "    \n",
    "    global num_epochs_without_learning\n",
    "\n",
    "\n",
    "    \n",
    "    #weights_before.append(model.get_weights())\n",
    "\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        #if epoch == 0:\n",
    "        #    weights_before.append([model.get_weights()])\n",
    "        \n",
    "        #if a and b and c and d:\n",
    "            \n",
    "        #    weights_after.append(model.get_weights())\n",
    "            \n",
    "        #    break\n",
    "        \n",
    "        #avg_loss_for_epoch = 0\n",
    "        epoch_grads = []\n",
    "        cur_epoch_context_loss[hot_context_idx] = 0\n",
    "        \n",
    "        \n",
    "        #======================#\n",
    "        # General Forward Pass #      \n",
    "        #----------------------#\n",
    "        test_forward_pass(dataset, model, epoch_grads, NTASK_LAYER_IDX, cur_epoch_context_loss, all_epoch_losses)\n",
    "\n",
    "        \n",
    "        # If gone num_task_contexts epochs without learning on a context\n",
    "        # And No Context Fits Well, Need To Pick Best Fit\n",
    "        if num_epochs_without_learning >= num_task_contexts:\n",
    "            \n",
    "            \n",
    "            # Find Best Fitting Context For Current Task\n",
    "            # bc went over all the contexts on the current task\n",
    "            # the diff_errs accurately tells us which context fits best for this task\n",
    "            next_context_idx = diff_errs.index(max(diff_errs))\n",
    "            \n",
    "            # Best fitting context is the one that just had a forward pass performed on it\n",
    "            # So -> Apply the Gradients\n",
    "            # Continue to next epoch \n",
    "            if next_context_idx == hot_context_idx:\n",
    "                diff_errs, num_epochs_without_learning = update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model)\n",
    "                \n",
    "                #a=True\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # Current Context does not have the best fit, so don't apply its grads\n",
    "            # Now that the best fitting context has been found, train on it\n",
    "            else:\n",
    "                \n",
    "                # Switch to best fitting Context\n",
    "                #hot_context_idx, epoch_loss = switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_error, epoch_loss)\n",
    "                hot_context_idx = switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_loss)\n",
    "\n",
    "                #==================#\n",
    "                # General Training #\n",
    "                #==================#\n",
    "                test_forward_pass(dataset, model, epoch_grads, NTASK_LAYER_IDX, cur_epoch_context_loss, all_epoch_losses)\n",
    "\n",
    "                #this is running an extra epoch, we need to keep track\n",
    "                double_epoch_count += 1\n",
    "            \n",
    "                diff_errs, num_epochs_without_learning = update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model)\n",
    "                \n",
    "                # b = True\n",
    "                continue\n",
    "        \n",
    "        next_context_idx = (hot_context_idx + 1) % len(moving_avg_context_loss)\n",
    "                \n",
    "        \n",
    "        diff_errs[hot_context_idx] = moving_avg_context_loss[hot_context_idx] - cur_epoch_context_loss[hot_context_idx]\n",
    "        \n",
    "        ##########################\n",
    "        # Should this happen if right context too?\n",
    "        # If so, where should this occur?\n",
    "        ###########################\n",
    "        if diff_errs[hot_context_idx] < context_switch_threshold:            \n",
    "            num_epochs_without_learning += 1\n",
    "            \n",
    "            \n",
    "            # TRACK WHICH EPOCH THIS BOCK HAPPENS IN\n",
    "            # PLOT ON GRAPH\n",
    "            # OVER TIME, THIS BLOCK SHOULD GET EXECUTED LESS\n",
    "            # SO WHEN TASKS ARE LEARNED WELL, SHOULD SEE THIS EXECTUED ONLY ONCE, FIRST EPOCH OF NEW TASK\n",
    "            \n",
    "            # find best context\n",
    "            # if it is my current context -> learn\n",
    "            # else switch to best \n",
    "            \n",
    "            #WE DONT LOSE EPOCHS HERE BY DOING IT THIS WAY            \n",
    "            \n",
    "            hot_context_idx = switch_to_next_context(next_context_idx, model, NTASK_LAYER_IDX, hot_context_idx)\n",
    "            \n",
    "            #c = True\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        #--------------------------    \n",
    "        # didnt switch, so apply grads\n",
    "        else:\n",
    "            #update moving avg err\n",
    "            moving_avg_context_loss[hot_context_idx] = (moving_avg_context_loss[hot_context_idx] + cur_epoch_context_loss[hot_context_idx]) / 2.0\n",
    "            \n",
    "            #d = True\n",
    "            \n",
    "            #for grads in epoch_grads:\n",
    "            #    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_testing_in_cycles(all_data, model, num_tasks, num_cycles, num_epochs):\n",
    "    \n",
    "    global epoch_accuracies\n",
    "    global double_epoch_count\n",
    "    global weights_after\n",
    "    global weights_before\n",
    "    \n",
    "    \n",
    "    global a\n",
    "    global b\n",
    "    global c\n",
    "    global d\n",
    "    \n",
    "    double_epoch_count = 0\n",
    "    \n",
    "    epoch_accuracies = []\n",
    "    \n",
    "    prev_task_data_idx = num_tasks-1    # init first choice as last task\n",
    "    order_of_tasks_learned_on = []\n",
    "\n",
    "    weights_before = []\n",
    "    weights_after = []\n",
    "    \n",
    "    #FYI this is correct:\n",
    "    # same as for c in range(cycle): for t in range num_tasks:\n",
    "    for i in tqdm(range( num_cycles * num_tasks )):\n",
    "    #while len(weights_after) == 0:\n",
    "    \n",
    "        cur_task_data_idx = randrange(num_tasks)\n",
    "\n",
    "        while cur_task_data_idx == prev_task_data_idx:\n",
    "            cur_task_data_idx = randrange(num_tasks)\n",
    "\n",
    "        cur_task_data = all_data[cur_task_data_idx]\n",
    "        #print(\"Currently training on data from all_data[ \", cur_task_data_idx)\n",
    "        order_of_tasks_learned_on.append(cur_task_data_idx)\n",
    "\n",
    "        \n",
    "        global test_switch_epoch_counter\n",
    "        test_switch_epoch_counter = 0\n",
    "        \n",
    "        test(model, cur_task_data, n_epochs=num_epochs, debug=False, plotting_debug=False, num_task_contexts=num_tasks)\n",
    "        \n",
    "     #   print(\"Len of weights before:\", len(weights_before))\n",
    "        \n",
    "      #  print(a, b, c, d)\n",
    "\n",
    "        prev_task_data_idx = cur_task_data_idx\n",
    "        \n",
    "    return order_of_tasks_learned_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For checking percent of correct epochs during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_correct_epochs(epoch_acc_list):\n",
    "    \n",
    "    \n",
    "    print( \"number of epochs:\", len( epoch_acc_list ) )\n",
    "    \n",
    "    correct_indices = []\n",
    "    \n",
    "    num_switches = 0\n",
    "    \n",
    "    num_correct = 0\n",
    "    for idx, acc in enumerate(epoch_acc_list):\n",
    "        if acc == 1.0:\n",
    "            num_correct += 1\n",
    "            correct_indices.append(idx)\n",
    "            \n",
    "        elif acc == \"switch\":\n",
    "            num_switches += 1\n",
    "    \n",
    "    \n",
    "    print(\"num_correct:\",num_correct)\n",
    "    print(len(correct_indices))\n",
    "    \n",
    "    print(\"num switches\", num_switches)\n",
    "    \n",
    "    return num_correct / ( float(len( epoch_acc_list )-num_switches) )\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accuracies = []\n",
    "double_epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_order = random_testing_in_cycles(all_data, model, num_tasks=num_task_contexts, num_cycles=10, num_epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(task_order)\n",
    "plt.ylabel(\"Task\")\n",
    "plt.xlabel(\"Num Cycles * Num Tasks\")\n",
    "plt.title(\"Pre-Training Task Order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_percent_correct_epochs = percent_correct_epochs( epoch_accuracies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_epoch_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_train_percent_correct_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3f6becfd16479eb448b959627ef107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\envs\\tensorflow_1.15\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_order = random_training_in_cycles(all_data, model, num_tasks=num_task_contexts, num_cycles=20, num_epochs=500)\n",
    "\n",
    "#task_order = random_training_in_cycles(all_data, model, num_tasks=num_task_contexts, num_cycles=400, num_epochs=500)\n",
    "\n",
    "#task_order = random_training_in_cycles(all_data, model, num_tasks=num_task_contexts, num_cycles=10, num_epochs=100)\n",
    "\n",
    "#training_percent_correct_epochs = percent_correct_epochs( epoch_accuracies )\n",
    "#training_percent_correct_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accuracies = []\n",
    "double_epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0bb5b760874feea46386185e752f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of epochs: 8043\n",
      "num_correct: 2526\n",
      "2526\n",
      "num switches 560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33756514766804757"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_order = random_testing_in_cycles(all_data, model, num_tasks=num_task_contexts, num_cycles=10, num_epochs=100)\n",
    "\n",
    "post_train_percent_correct_epochs = percent_correct_epochs( epoch_accuracies )\n",
    "post_train_percent_correct_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(task_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(task_order)\n",
    "plt.ylabel(\"Task\")\n",
    "plt.xlabel(\"Num Cycles * Num Tasks\")\n",
    "plt.title(\"Post-Training Task Order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ('Pre-Train Testing', 'Post-Train Testing')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [pre_train_percent_correct_epochs, post_train_percent_correct_epochs]\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('% Correct Epochs')\n",
    "plt.title('{} Logic Gates'.format(num_task_contexts))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[NTASK_LAYER_IDX].set_hot_context(hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in all_data[1]:\n",
    "    print(model.predict(i[0]) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [int(x+1) for x in range(len(all_epoch_losses))]\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epoch_losses_as_float = [x.numpy()[0] for x in all_epoch_losses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using a log plot\n",
    "#### use a logarithmic transform log_base10 is fine\n",
    "#### compresses top but leaves bottom the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(100, 10))\n",
    "\n",
    "sns.lineplot(x=epochs, y=all_epoch_losses_as_float).set_title(\"Title\", fontsize=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num_contexts, model, cont_idx):\n",
    "    for i in range(num_contexts):\n",
    "        model.layers[cont_idx].set_hot_context(i)\n",
    "        a, b = logic_gate_test.test(model)\n",
    "        \n",
    "        print(b)\n",
    "        #return a, b\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "\n",
      "[array([[1.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "[array([[1.]], dtype=float32), array([[0.]], dtype=float32), array([[1.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "\n",
      "[array([[0.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "[array([[0.]], dtype=float32), array([[1.]], dtype=float32), array([[0.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "[array([[0.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "\n",
      "[array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "[array([[1.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(num_task_contexts, model, NTASK_LAYER_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0a1df294b701>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_preds, rounded_preds = logic_gate_test.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_task_preds(preds):\n",
    "    \n",
    "    return [ preds[idx][0][0] for idx, x in enumerate(preds) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_raw_and_rounded_preds(num_tasks, cont_idx, model):\n",
    "    all_raw_preds = []\n",
    "    all_rounded_preds = []\n",
    "\n",
    "    for i in range(num_tasks):\n",
    "        model.layers[cont_idx].set_hot_context(i)\n",
    "\n",
    "        raw_preds, rounded_preds = logic_gate_test.test(model)\n",
    "\n",
    "        raw_preds = consolidate_task_preds(raw_preds)\n",
    "        rounded_preds = consolidate_task_preds(rounded_preds)\n",
    "\n",
    "        all_raw_preds.append( raw_preds )\n",
    "        all_rounded_preds.append( rounded_preds )\n",
    "\n",
    "    return all_raw_preds, all_rounded_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(all_rounded_preds):\n",
    "    set_rounded_preds = set(tuple(x) for x in all_rounded_preds)\n",
    "    dups_removed_rounded_preds = [ list(x) for x in set_rounded_preds ]\n",
    "    dups_removed_rounded_preds.sort(key = lambda x: all_rounded_preds.index(x) )\n",
    "    \n",
    "    return dups_removed_rounded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_over_all_tasks(all_rounded_preds, dups_removed_rounded_preds, labels, num_tasks):\n",
    "\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Find accuracy over all tasks.\n",
    "        For a task to be considered correct, it must produce the EXACT right output.\n",
    "        Producing the EXACT right output more than once only counts correct once.\n",
    "        If labels are: [ [a, b], [a, c], [b, c] ] and model produces: [ [a, b], [a, b], [a, a] ] -> 33% accurate\n",
    "        If labels are: [ [a, b], [a, c], [b, c] ] and model produces: [ [a, b], [a, c], [a, a] ] -> 66% accurate\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    num_correct_duplicates = len(all_rounded_preds) - len(dups_removed_rounded_preds)\n",
    "\n",
    "    num_wrong = 0\n",
    "\n",
    "    for i in range(len(dups_removed_rounded_preds)):\n",
    "        if dups_removed_rounded_preds[i] not in labels:\n",
    "            num_wrong += 1\n",
    "\n",
    "    num_wrong += num_correct_duplicates\n",
    "\n",
    "    num_correct = num_tasks - num_wrong\n",
    "\n",
    "    acc = ( num_correct / num_tasks ) * 100\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(all_data):\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(all_data)):\n",
    "\n",
    "        label = []\n",
    "        for inp in all_data[i]:\n",
    "            label.append(  inp[-1] )\n",
    "\n",
    "        labels.append(label)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_preds, all_rounded_preds = get_all_raw_and_rounded_preds(num_task_contexts, NTASK_LAYER_IDX, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_duplicates_all_rounded_preds = remove_duplicates(all_rounded_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = get_accuracy_over_all_tasks(all_rounded_preds, no_duplicates_all_rounded_preds, labels, num_task_contexts)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_set = set(all_rounded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    if all_rounded_preds[i] not in labels:\n",
    "        print(\"Not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_all_tasks_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = consolidate_preds(rounded_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test(num_task_contexts, model, NTASK_LAYER_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to test model, need to see if learned all representations 1 TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(model, labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
