{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widen jupyter notebook \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from Context_Layer import Context as Ntask\n",
    "\n",
    "# Graphing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Seeding\n",
    "from random import randrange \n",
    "import random\n",
    "random.seed(5)\n",
    "tf.set_random_seed(5)\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "# Datasets\n",
    "import dataset_8_logic_gates as data\n",
    "import logic_gate_test\n",
    "\n",
    "# General\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn off GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 logic gates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data.all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context layer index in neural network\n",
    "NTASK_LAYER_IDX = 2\n",
    "\n",
    "# We want a context for each task we want the network to learn\n",
    "num_task_contexts=len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(2,)\n",
    "x = Dense(20, activation='relu')(inp)\n",
    "x = Ntask(num_task_contexts, hardcoded_contexts=False)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "context (Context)            (None, 20)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 241\n",
      "Trainable params: 81\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function is Binary Crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, predictions):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true=labels, y_pred=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer is Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Switching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_loss):\n",
    "\n",
    "    \"\"\"\n",
    "    Swith Ntask layer to better fitting context, when provided better fitting context.\n",
    "    Resets several variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    # hot_context_idx is the index of the context that is currently \"on\" within the context layer\n",
    "    hot_context_idx = next_context_idx\n",
    "    \n",
    "    # switch the context within the context layer to be that associated with hot_context_idx\n",
    "    model.layers[NTASK_LAYER_IDX].set_hot_context( hot_context_idx )\n",
    "    \n",
    "    # Reset vars\n",
    "    epoch_grads.clear()\n",
    "    cur_epoch_context_loss[hot_context_idx] = 0\n",
    "    \n",
    "    return hot_context_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def switch_to_next_context(next_context_idx, model, context_idx, hot_context_idx):\n",
    "    \n",
    "    \"\"\"\n",
    "    Switch Ntask layer hot context to the next context & update hot_context_idx.\n",
    "    ex: If hot_context_idx := 2 \n",
    "            func call results in:\n",
    "                hot_context_idx := 3\n",
    "                the hot context is now the context at idx 3\n",
    "    \"\"\"\n",
    "    # hot_context_idx of the index of the context that is currently \"on\" within the context layer\n",
    "    hot_context_idx = next_context_idx\n",
    "    \n",
    "    # switch the context within the context layer to be that associated with hot_context_idx\n",
    "    model.layers[NTASK_LAYER_IDX].set_hot_context( hot_context_idx )\n",
    "\n",
    "    return hot_context_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_context_loss(gradients, model, ntask_layer_idx_in_model, idx_of_next_layer_bias_gradient, idx_of_next_layer_weights_in_get_weights_call=0):\n",
    "    \"\"\"\n",
    "    IMPORTANT: \n",
    "    1) Assumes no use of activation function on Ntask layer\n",
    "    2) Assumes that the layer following the Ntask layer:\n",
    "        a) Is a Dense layer\n",
    "        b) Is using bias \n",
    "           — ex: Dense(20, ... , use_bias=True) \n",
    "           — note Keras Dense layer uses bias by default if no value is given for use_bias param\n",
    "    3) Assumes index of the next layer's gradient is known within the gradients list returned from gradient tape in a tape.gradient call\n",
    "    4) If the above points aren't met, things will break and it may be hard to locate the bugs\n",
    "    \"\"\"\n",
    "    # from the delta rule in neural network math\n",
    "    delta_at_next_layer = gradients[idx_of_next_layer_bias_gradient]\n",
    "    transpose_of_weights_at_next_layer = tf.transpose(model.layers[ntask_layer_idx_in_model+1].get_weights()[idx_of_next_layer_weights_in_get_weights_call])\n",
    "      \n",
    "    # Calculate delta at ntask layer\n",
    "    context_delta = np.dot( delta_at_next_layer, transpose_of_weights_at_next_layer ).astype(np.float)\n",
    "    \n",
    "    # Calculate Context Error\n",
    "    # Keras MSE must have both args be arrs of floats, if one or both are arrs of ints, the output will be rounded to an int\n",
    "    # This is how responsible the context layer was for the loss\n",
    "    context_loss = tf.keras.losses.mean_squared_error(np.zeros(len(context_delta)), context_delta)\n",
    "\n",
    "    return context_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_forward_pass(dataset, model, epoch_grads, context_idx, cur_epoch_context_loss, all_epoch_losses):\n",
    "\n",
    "    \"\"\"\n",
    "    This is the training forward pass for an entire epoch\n",
    "    \n",
    "    !!!!! Read through this code as it is a nonstandard training forward pass ( different than model.fit() )\n",
    "    & NOTE that this does not apply the gradients ie. this does not do a weight update/learn\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    sum_loss = 0\n",
    "    \n",
    "    # Tensorflow 2 style training -- info can be found here: https://www.tensorflow.org/guide/effective_tf2 \n",
    "    # This is similar to model.fit(), however this is a custom training loop -- ie. it does things differently than model.fit()\n",
    "    # look at each input and label (there are 4 for the logic gates)\n",
    "    for x, y in dataset:   \n",
    "\n",
    "        # Get the prediction and loss for the current sample\n",
    "        # This needs to be under the GradientTape call to extract the gradients a couple of lines below...\n",
    "        with tf.GradientTape(persistent=True) as tape:            \n",
    "            predictions = model(x, training=True) # forward pass\n",
    "            pred_loss = loss_fn(y, predictions)   # get loss\n",
    "\n",
    "        \n",
    "        # We will be using a sum of the loss for the epoch before updating weights\n",
    "        sum_loss += pred_loss\n",
    "        \n",
    "        # Extract the gradients for the loss of the current sample\n",
    "        gradients = tape.gradient(pred_loss, model.trainable_variables)\n",
    "        \n",
    "        # We collect the gradients from each sample in the dataset for the epoch\n",
    "        epoch_grads.append(gradients)\n",
    "\n",
    "        # How responsbile the context layer was for the loss\n",
    "        context_loss = calc_context_loss(gradients, \n",
    "                                           model, \n",
    "                                           context_idx, \n",
    "                                           idx_of_next_layer_bias_gradient=3, \n",
    "                                           idx_of_next_layer_weights_in_get_weights_call=0)\n",
    "        \n",
    "        # Accumulating the context loss\n",
    "        cur_epoch_context_loss[hot_context_idx] += context_loss\n",
    "\n",
    "    # avg loss for epoch\n",
    "    avg_loss_for_epoch = sum_loss / len(dataset)\n",
    "\n",
    "    # Save the epoch losses\n",
    "    all_epoch_losses.append(avg_loss_for_epoch)\n",
    "\n",
    "    \n",
    "    \n",
    "    return  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_grads_and_update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This is where a weight update/learning/applying gradients occurs\n",
    "    \n",
    "    Some conditional variables are also updated\n",
    "    \n",
    "    This is called when it has been decided that the current context is good enough to learn on\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Update moving avg context loss\n",
    "    moving_avg_context_loss[hot_context_idx] = (moving_avg_context_loss[hot_context_idx] + cur_epoch_context_loss[hot_context_idx]) / 2.0\n",
    "    \n",
    "    # Reset the diff_errs\n",
    "    diff_errs = [0 for x in range(num_task_contexts)]\n",
    "\n",
    "    \n",
    "    # Backprop\n",
    "    # We apply all the gradients from the epoch \n",
    "    # I think there are gradients from each sample which is ( !!!! nonstandard  )\n",
    "    # As opposed to like average gradients over a batch\n",
    "    for grads in epoch_grads:\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Reset counter\n",
    "    num_epochs_without_learning = 0\n",
    "\n",
    "    return diff_errs, num_epochs_without_learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for context switching behavior\n",
    "thresh = 1.0\n",
    "moving_avg_context_loss = [thresh for x in range(num_task_contexts)]\n",
    "\n",
    "# init vals before training\n",
    "hot_context_idx = 0\n",
    "diff_errs = [thresh for x in range(num_task_contexts)]\n",
    "cur_epoch_context_loss = [0 for x in range(num_task_contexts)]\n",
    "all_epoch_losses = []\n",
    "context_switch_threshold = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_without_learning = 0\n",
    "\n",
    "def train(model, dataset, n_epochs, debug, plotting_debug, num_task_contexts):\n",
    "    \n",
    "    \n",
    "    # Global variables are used for sharing things\n",
    "    # Some of these values are mutated in functions and not seen as return values\n",
    "    global all_epoch_losses\n",
    "    global hot_context_idx\n",
    "    global cur_epoch_context_loss\n",
    "    global moving_avg_context_loss\n",
    "    global diff_errs\n",
    "    global context_switch_threshold\n",
    "    global num_epochs_without_learning\n",
    "\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # init\n",
    "        epoch_grads = []\n",
    "        cur_epoch_context_loss[hot_context_idx] = 0\n",
    "        \n",
    "        \n",
    "        #=====================================================================#\n",
    "        # Nonstandard Forward Pass -- this is for every sample in the dataset #      \n",
    "        #---------------------------------------------------------------------#\n",
    "        custom_forward_pass(dataset, model, epoch_grads, NTASK_LAYER_IDX, cur_epoch_context_loss, all_epoch_losses)\n",
    "\n",
    "        \n",
    "        # If gone num_task_contexts epochs without learning on a context\n",
    "        # And No Context Fits Well, Need To Pick Best Fit\n",
    "        if num_epochs_without_learning >= num_task_contexts:\n",
    "            \n",
    "            \n",
    "            # Find Best Fitting Context For Current Task\n",
    "            # because went over all the contexts on the current task\n",
    "            # the diff_errs tells us which context fits best for this task\n",
    "            next_context_idx = diff_errs.index(max(diff_errs))\n",
    "            \n",
    "            # Best fitting context is the one that just had a forward pass performed on it\n",
    "            # So -> Apply the Gradients\n",
    "            # Continue to next epoch \n",
    "            if next_context_idx == hot_context_idx:\n",
    "                diff_errs, num_epochs_without_learning = apply_grads_and_update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model)\n",
    "                continue\n",
    "            \n",
    "            # Current Context does not have the best fit, so don't apply its grads\n",
    "            # Now that the best fitting context has been found, train on it\n",
    "            else:\n",
    "                \n",
    "                # Switch to best fitting Context\n",
    "                #hot_context_idx, epoch_loss = switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_error, epoch_loss)\n",
    "                hot_context_idx = switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_loss)\n",
    "\n",
    "                #=====================================================================#\n",
    "                # Nonstandard Forward Pass -- this is for every sample in the dataset #      \n",
    "                #---------------------------------------------------------------------#\n",
    "                custom_forward_pass(dataset, model, epoch_grads, NTASK_LAYER_IDX, cur_epoch_context_loss, all_epoch_losses)\n",
    "\n",
    "                #Apply the Gradients\n",
    "                #continue\n",
    "                diff_errs, num_epochs_without_learning = apply_grads_and_update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model)\n",
    "                continue\n",
    "        \n",
    "        # get the next context index in line: ie if at 0 get 1 \n",
    "        next_context_idx = (hot_context_idx + 1) % len(moving_avg_context_loss)\n",
    "                \n",
    "        # very important for switching behavior\n",
    "        # a kind of dynamic threshold\n",
    "        diff_errs[hot_context_idx] = moving_avg_context_loss[hot_context_idx] - cur_epoch_context_loss[hot_context_idx]\n",
    "        \n",
    "\n",
    "        # our dynamic threshold hasn't passed the point where we switch\n",
    "        if diff_errs[hot_context_idx] < context_switch_threshold:       \n",
    "            \n",
    "            # we want to count how many epochs we don't learn on\n",
    "            # we use this to check that we haven't gone through every context without learning\n",
    "            # otherwise we could keep performing epochs that never learn forever\n",
    "            num_epochs_without_learning += 1\n",
    "            \n",
    "                      \n",
    "            # go to the next context in line; if at context 0 go to context 1\n",
    "            hot_context_idx = switch_to_next_context(next_context_idx, model, NTASK_LAYER_IDX, hot_context_idx)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        #--------------------------    \n",
    "        # didnt switch, so must have been on correct contest already, so apply grads\n",
    "        else:\n",
    "            #update moving avg err\n",
    "            moving_avg_context_loss[hot_context_idx] = (moving_avg_context_loss[hot_context_idx] + cur_epoch_context_loss[hot_context_idx]) / 2.0\n",
    "            \n",
    "            # learn on the gradients (nonstandard approach)\n",
    "            for grads in epoch_grads:\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Randomly on tasks for N cycles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to randomly throw tasks at the model while it is dynamically training.\n",
    "SO the model has to switch its context on the fly while training before any contexts have fully learned a task mapped to a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_in_cycles(all_data, model, num_tasks, num_cycles, num_epochs):\n",
    "    \n",
    "    prev_task_data_idx = num_tasks-1    # init first choice as last task\n",
    "    order_of_tasks_learned_on = []\n",
    "\n",
    "    #FYI this is correct:\n",
    "    # same as for c in range(cycle): for t in range num_tasks:\n",
    "    for i in tqdm(range( num_cycles * num_tasks )):\n",
    "\n",
    "        cur_task_data_idx = randrange(num_tasks)\n",
    "\n",
    "        # Don't learn on the same task as the previous time\n",
    "        while cur_task_data_idx == prev_task_data_idx:\n",
    "            cur_task_data_idx = randrange(num_tasks)\n",
    "\n",
    "        # get current task data\n",
    "        cur_task_data = all_data[cur_task_data_idx]\n",
    "\n",
    "        # so we can see the order of tasks after training\n",
    "        order_of_tasks_learned_on.append(cur_task_data_idx)\n",
    "\n",
    "\n",
    "        # train on the current task for specified number of epochs \n",
    "        train(model, cur_task_data, n_epochs=num_epochs, debug=False, plotting_debug=False, num_task_contexts=num_tasks)\n",
    "\n",
    "        prev_task_data_idx = cur_task_data_idx\n",
    "        \n",
    "    return order_of_tasks_learned_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ****Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3f035618c54a14827e73fefad4f452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=160.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda\\envs\\tensorflow_1.15\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task_order = random_training_in_cycles(all_data, model, num_tasks=num_task_contexts, num_cycles=20, num_epochs=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Has the modeled learned the 8 logic gates mapped to its 8 contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num_contexts, model, cont_idx):\n",
    "    for i in range(num_contexts):\n",
    "        model.layers[cont_idx].set_hot_context(i)\n",
    "        a, b = logic_gate_test.test(model)\n",
    "        \n",
    "        print(b)\n",
    "        #return a, b\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yes it has\n",
    "#### each line is the rounded output for a task\n",
    "#### XOR looks like: 0 1 1 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "\n",
      "[array([[1.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "[array([[1.]], dtype=float32), array([[0.]], dtype=float32), array([[1.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "\n",
      "[array([[0.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "[array([[0.]], dtype=float32), array([[1.]], dtype=float32), array([[0.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "[array([[0.]], dtype=float32), array([[1.]], dtype=float32), array([[1.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "\n",
      "[array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[1.]], dtype=float32)]\n",
      "\n",
      "[array([[1.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32), array([[0.]], dtype=float32)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(num_task_contexts, model, NTASK_LAYER_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's another way of looking to see if the network has learned to map the 8 tasks to its 8 different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_task_preds(preds):\n",
    "    \n",
    "    return [ preds[idx][0][0] for idx, x in enumerate(preds) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_raw_and_rounded_preds(num_tasks, cont_idx, model):\n",
    "    all_raw_preds = []\n",
    "    all_rounded_preds = []\n",
    "\n",
    "    for i in range(num_tasks):\n",
    "        model.layers[cont_idx].set_hot_context(i)\n",
    "\n",
    "        raw_preds, rounded_preds = logic_gate_test.test(model)\n",
    "\n",
    "        raw_preds = consolidate_task_preds(raw_preds)\n",
    "        rounded_preds = consolidate_task_preds(rounded_preds)\n",
    "\n",
    "        all_raw_preds.append( raw_preds )\n",
    "        all_rounded_preds.append( rounded_preds )\n",
    "\n",
    "    return all_raw_preds, all_rounded_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(all_rounded_preds):\n",
    "    set_rounded_preds = set(tuple(x) for x in all_rounded_preds)\n",
    "    dups_removed_rounded_preds = [ list(x) for x in set_rounded_preds ]\n",
    "    dups_removed_rounded_preds.sort(key = lambda x: all_rounded_preds.index(x) )\n",
    "    \n",
    "    return dups_removed_rounded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_over_all_tasks(all_rounded_preds, dups_removed_rounded_preds, labels, num_tasks):\n",
    "\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Find accuracy over all tasks.\n",
    "        For a task to be considered correct, it must produce the EXACT right output.\n",
    "        Producing the EXACT right output more than once only counts correct once.\n",
    "        If labels are: [ [a, b], [a, c], [b, c] ] and model produces: [ [a, b], [a, b], [a, a] ] -> 33% accurate\n",
    "        If labels are: [ [a, b], [a, c], [b, c] ] and model produces: [ [a, b], [a, c], [a, a] ] -> 66% accurate\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    num_correct_duplicates = len(all_rounded_preds) - len(dups_removed_rounded_preds)\n",
    "\n",
    "    num_wrong = 0\n",
    "\n",
    "    for i in range(len(dups_removed_rounded_preds)):\n",
    "        if dups_removed_rounded_preds[i] not in labels:\n",
    "            num_wrong += 1\n",
    "\n",
    "    num_wrong += num_correct_duplicates\n",
    "\n",
    "    num_correct = num_tasks - num_wrong\n",
    "\n",
    "    acc = ( num_correct / num_tasks ) * 100\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(all_data):\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(all_data)):\n",
    "\n",
    "        label = []\n",
    "        for inp in all_data[i]:\n",
    "            label.append(  inp[-1] )\n",
    "\n",
    "        labels.append(label)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_raw_preds, all_rounded_preds = get_all_raw_and_rounded_preds(num_task_contexts, NTASK_LAYER_IDX, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_duplicates_all_rounded_preds = remove_duplicates(all_rounded_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels(all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100% accurate - ie. learned the 8 tasks mapped to its 8 different contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = get_accuracy_over_all_tasks(all_rounded_preds, no_duplicates_all_rounded_preds, labels, num_task_contexts)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The above type of testing could be thought of as STATIC testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# However, we want to have dynamic testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where things start to break down..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic testing is basically the above dynamic training code with all of the learning parts removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic testing functions derived from training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model):\n",
    "\n",
    "    moving_avg_context_loss[hot_context_idx] = (moving_avg_context_loss[hot_context_idx] + cur_epoch_context_loss[hot_context_idx]) / 2.0\n",
    "    \n",
    "    diff_errs = [0 for x in range(num_task_contexts)]\n",
    "\n",
    "    #Backprop\n",
    "    #for grads in epoch_grads:\n",
    "    #    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    num_epochs_without_learning = 0\n",
    "\n",
    "    return diff_errs, num_epochs_without_learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_accuracies = []\n",
    "double_epoch_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "global test_switch_epoch_counter\n",
    "\n",
    "test_switch_epoch_counter = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass(dataset, model, epoch_grads, context_idx, cur_epoch_context_loss, all_epoch_losses):\n",
    " \n",
    "    global epoch_accuracies\n",
    "    \n",
    "    global test_switch_epoch_counter\n",
    "    \n",
    "    if  ( not np.isnan(test_switch_epoch_counter) ) and ( test_switch_epoch_counter < num_task_contexts-1 ):\n",
    "        test_switch_epoch_counter += 1\n",
    "        #append switch\n",
    "        epoch_accuracies.append(\"switch\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    if test_switch_epoch_counter >= num_task_contexts-1:\n",
    "        test_switch_epoch_counter = np.nan\n",
    "    \n",
    "    \n",
    "    sum_loss = 0\n",
    "    \n",
    "    acc = 0.0\n",
    "    #print(\"acc at top of loop:\", acc)\n",
    "    #print()\n",
    "    for x, y in dataset:   \n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:            \n",
    "            predictions = model(x, training=True) # forward pass\n",
    "            pred_loss = loss_fn(y, predictions)   # get loss\n",
    "\n",
    "        rounded_pred = int(tf.math.round(predictions).numpy()[0][0])\n",
    "            \n",
    "        #print(\"predictions:\", rounded_pred )   \n",
    "        #print(\"label:\", y)\n",
    "        \n",
    "        #print()\n",
    "        \n",
    "        if rounded_pred == y:\n",
    "            acc += 1.0\n",
    "            \n",
    "        sum_loss += pred_loss\n",
    "        \n",
    "        gradients = tape.gradient(pred_loss, model.trainable_variables)\n",
    "        epoch_grads.append(gradients)\n",
    "\n",
    "        context_loss = calc_context_loss(gradients, \n",
    "                                           model, \n",
    "                                           context_idx, \n",
    "                                           idx_of_next_layer_bias_gradient=3, \n",
    "                                           idx_of_next_layer_weights_in_get_weights_call=0)\n",
    "        \n",
    "        \n",
    "        cur_epoch_context_loss[hot_context_idx] += context_loss\n",
    "    \n",
    "    acc /= len(dataset)\n",
    "    #print(\"acc at end of epoch:\", acc)\n",
    "\n",
    "    epoch_accuracies.append(acc)\n",
    "    \n",
    "    avg_loss_for_epoch = sum_loss / len(dataset)\n",
    "\n",
    "    all_epoch_losses.append(avg_loss_for_epoch)\n",
    "\n",
    "    return  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General dynamic testing derived from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_without_learning = 0\n",
    "\n",
    "def test(model, dataset, n_epochs, debug, plotting_debug, num_task_contexts):\n",
    "\n",
    "    #global weights_before\n",
    "    #global weights_after\n",
    "    \n",
    "    global double_epoch_count\n",
    "    \n",
    "    #global a\n",
    "    #global b\n",
    "    #global c\n",
    "    #global d\n",
    "    \n",
    "    global all_epoch_losses\n",
    "\n",
    "    global hot_context_idx\n",
    "    \n",
    "    global cur_epoch_context_loss\n",
    "    global moving_avg_context_loss\n",
    "    global diff_errs\n",
    "    global context_switch_threshold\n",
    "    \n",
    "    global num_epochs_without_learning\n",
    "\n",
    "\n",
    "    \n",
    "    #weights_before.append(model.get_weights())\n",
    "\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        #if epoch == 0:\n",
    "        #    weights_before.append([model.get_weights()])\n",
    "        \n",
    "        #if a and b and c and d:\n",
    "            \n",
    "        #    weights_after.append(model.get_weights())\n",
    "            \n",
    "        #    break\n",
    "        \n",
    "        #avg_loss_for_epoch = 0\n",
    "        epoch_grads = []\n",
    "        cur_epoch_context_loss[hot_context_idx] = 0\n",
    "        \n",
    "        \n",
    "        #======================#\n",
    "        # General Forward Pass #      \n",
    "        #----------------------#\n",
    "        test_forward_pass(dataset, model, epoch_grads, NTASK_LAYER_IDX, cur_epoch_context_loss, all_epoch_losses)\n",
    "\n",
    "        \n",
    "        # If gone num_task_contexts epochs without learning on a context\n",
    "        # And No Context Fits Well, Need To Pick Best Fit\n",
    "        if num_epochs_without_learning >= num_task_contexts:\n",
    "            \n",
    "            \n",
    "            # Find Best Fitting Context For Current Task\n",
    "            # bc went over all the contexts on the current task\n",
    "            # the diff_errs accurately tells us which context fits best for this task\n",
    "            next_context_idx = diff_errs.index(max(diff_errs))\n",
    "            \n",
    "            # Best fitting context is the one that just had a forward pass performed on it\n",
    "            # So -> Apply the Gradients\n",
    "            # Continue to next epoch \n",
    "            if next_context_idx == hot_context_idx:\n",
    "                diff_errs, num_epochs_without_learning = update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model)\n",
    "                \n",
    "                #a=True\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            # Current Context does not have the best fit, so don't apply its grads\n",
    "            # Now that the best fitting context has been found, train on it\n",
    "            else:\n",
    "                \n",
    "                # Switch to best fitting Context\n",
    "                #hot_context_idx, epoch_loss = switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_error, epoch_loss)\n",
    "                hot_context_idx = switch_to_better_fitting_context(next_context_idx, model, NTASK_LAYER_IDX, epoch_grads, cur_epoch_context_loss)\n",
    "\n",
    "                #==================#\n",
    "                # General Training #\n",
    "                #==================#\n",
    "                test_forward_pass(dataset, model, epoch_grads, NTASK_LAYER_IDX, cur_epoch_context_loss, all_epoch_losses)\n",
    "\n",
    "                #this is running an extra epoch, we need to keep track\n",
    "                double_epoch_count += 1\n",
    "            \n",
    "                diff_errs, num_epochs_without_learning = update_conditional_vars_etc(moving_avg_context_loss, hot_context_idx, cur_epoch_context_loss, epoch_grads, optimizer, model)\n",
    "                \n",
    "                # b = True\n",
    "                continue\n",
    "        \n",
    "        next_context_idx = (hot_context_idx + 1) % len(moving_avg_context_loss)\n",
    "                \n",
    "        \n",
    "        diff_errs[hot_context_idx] = moving_avg_context_loss[hot_context_idx] - cur_epoch_context_loss[hot_context_idx]\n",
    "        \n",
    "\n",
    "        if diff_errs[hot_context_idx] < context_switch_threshold:            \n",
    "            num_epochs_without_learning += 1\n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "            hot_context_idx = switch_to_next_context(next_context_idx, model, NTASK_LAYER_IDX, hot_context_idx)\n",
    "            \n",
    "            #c = True\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        #--------------------------    \n",
    "        # didnt switch, so apply grads\n",
    "        else:\n",
    "            #update moving avg err\n",
    "            moving_avg_context_loss[hot_context_idx] = (moving_avg_context_loss[hot_context_idx] + cur_epoch_context_loss[hot_context_idx]) / 2.0\n",
    "            \n",
    "            #d = True\n",
    "            \n",
    "            #for grads in epoch_grads:\n",
    "            #    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing loop derived from training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_testing_in_cycles(all_data, model, num_tasks, num_cycles, num_epochs):\n",
    "    \n",
    "    global epoch_accuracies\n",
    "    global double_epoch_count\n",
    "    global weights_after\n",
    "    global weights_before\n",
    "    \n",
    "    \n",
    "    global a\n",
    "    global b\n",
    "    global c\n",
    "    global d\n",
    "    \n",
    "    double_epoch_count = 0\n",
    "    \n",
    "    epoch_accuracies = []\n",
    "    \n",
    "    prev_task_data_idx = num_tasks-1    # init first choice as last task\n",
    "    order_of_tasks_learned_on = []\n",
    "\n",
    "    weights_before = []\n",
    "    weights_after = []\n",
    "    \n",
    "    #FYI this is correct:\n",
    "    # same as for c in range(cycle): for t in range num_tasks:\n",
    "    for i in tqdm(range( num_cycles * num_tasks )):\n",
    "    #while len(weights_after) == 0:\n",
    "    \n",
    "        cur_task_data_idx = randrange(num_tasks)\n",
    "\n",
    "        while cur_task_data_idx == prev_task_data_idx:\n",
    "            cur_task_data_idx = randrange(num_tasks)\n",
    "\n",
    "        cur_task_data = all_data[cur_task_data_idx]\n",
    "        #print(\"Currently training on data from all_data[ \", cur_task_data_idx)\n",
    "        order_of_tasks_learned_on.append(cur_task_data_idx)\n",
    "\n",
    "        \n",
    "        global test_switch_epoch_counter\n",
    "        test_switch_epoch_counter = 0\n",
    "        \n",
    "        test(model, cur_task_data, n_epochs=num_epochs, debug=False, plotting_debug=False, num_task_contexts=num_tasks)\n",
    "        \n",
    "     #   print(\"Len of weights before:\", len(weights_before))\n",
    "        \n",
    "      #  print(a, b, c, d)\n",
    "\n",
    "        prev_task_data_idx = cur_task_data_idx\n",
    "        \n",
    "    return order_of_tasks_learned_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For checking percent of correct epochs during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_correct_epochs(epoch_acc_list):\n",
    "    \n",
    "    \n",
    "    print( \"number of epochs:\", len( epoch_acc_list ) )\n",
    "    \n",
    "    correct_indices = []\n",
    "    \n",
    "    num_switches = 0\n",
    "    \n",
    "    num_correct = 0\n",
    "    for idx, acc in enumerate(epoch_acc_list):\n",
    "        if acc == 1.0:\n",
    "            num_correct += 1\n",
    "            correct_indices.append(idx)\n",
    "            \n",
    "        elif acc == \"switch\":\n",
    "            num_switches += 1\n",
    "    \n",
    "    \n",
    "    print(\"num_correct:\",num_correct)\n",
    "    print(len(correct_indices))\n",
    "    \n",
    "    print(\"num switches\", num_switches)\n",
    "    \n",
    "    return num_correct / ( float(len( epoch_acc_list )-num_switches) )\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the dynamic testing after we trained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8c88f6c41c4bc6b2719e426ead795a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=80.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "task_order = random_testing_in_cycles(all_data, model, num_tasks=num_task_contexts, num_cycles=10, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the percent of correct epochs from the dynamic testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33% acc on dynamic testing, even though we saw above that the model has learned to map the 8 tasks to its 8 contexts. We would expecte MUCH higher dynamic testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of epochs: 8043\n",
      "num_correct: 2526\n",
      "2526\n",
      "num switches 560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33756514766804757"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_train_percent_correct_epochs = percent_correct_epochs( epoch_accuracies )\n",
    "post_train_percent_correct_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that we expected 8000 epochs, but there were actually 8043 with our double epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of epochs should be num_cycles*num_epochs, however one of the branches in the training/testing adds extra epochs on the fly\n",
    "double_epoch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually inspect to see how the model did during testing\n",
    "# A perfect model (post_train_percent_correct_epochs := 1.0 ) would look like :\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "1.0\n",
    "...\n",
    "1.0\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "\"switch\"\n",
    "1.0\n",
    "...\n",
    "1.0\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.5,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.75,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 'switch',\n",
       " 0.25,\n",
       " 0.5,\n",
       " 0.5,\n",
       " 0.0,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The accuracies for each epoch\n",
    "# 0 -> none of the predictions for the 4 logic gates were correct\n",
    "# 0.25 -> one of the predictions for the 4 logic gates were correct\n",
    "# 0.5 -> two of the predictions for the 4 logic gates were correct\n",
    "# 0.75 -> three of the predictions for the 4 logic gates were correct\n",
    "# 1.0 -> all of the predictions for the 4 logic gates were correct\n",
    "\n",
    "# When a task has switched, we append \"switch\" for num_tasks - 1 epochs\n",
    "# The desired behavior is that after the n-1 switches, the model is now on the correct context and doesn't switch its context until the task switches\n",
    "# We don't count the n-1 \"switch\" epochs in the computation of accuracy here\n",
    "# This doesn't seem to happen as desired...\n",
    "epoch_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** Where might errors lie?\n",
    "- maybe in the diff_errs and moving_avg_context_loss\n",
    "    - this is a kind of thresholding system, which may work for training but not work for testing\n",
    "    - potentially, with the existing code, if the model was trained better, it could be overfit to each of the 8 tasks which would hopefully allow it to switch faster and stay on the correct context\n",
    "    \n",
    "- maybe an issue with sharing or not sharing global variables between training and testing (ex: diff_errs, moving_avg_context_loss, etc.)\n",
    "    - these maybe could be thought of as \"learned weights\" and are getting shared incorrectly or reset etc before testing.\n",
    "    \n",
    "- maybe the model needs to be trained better\n",
    "\n",
    "- maybe there is a better way to trigger task switching and perseverating\n",
    "   \n",
    "- maybe dynamic testing was implemented incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
